2023-06-03  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* eval:
	emrecanacikgoz/robot-language clone main branch
	cd evaluation
	read README
	downgrade numpy (create calvin-eval env)
	train-bc.py: training script for behavior cloning
	policies/behavior_cloning.py: model definition
	lightning model needs a step method step(self, state, task_id) => action_pred[7D] xyz, abc, gripper[-1,+1]
	experiments/ => model gets saved
	cd evaluation/ => do the evaluation here
	model file is copied here
	evaluate_multistep.py
	evaluate_singlestep.py: we are using this now
	needs TRAIN_FOLDER: simulator config file is here
	DATASET_PATH: /datasets/calvin/D/
	CONF_DIR: points to calvin directory
	DEBUG=True: saves videos
	ENV=None: use their environment, not our own.
	checkpoint: use your model
	import model.py file in the beginning
	model = Transformer.load_from_checkpoint()
	model.cuda: we need to do it?
	evaluation/utils.py: TASK_TO_ID_DICT: frame-ids in D that corresponds to particular task.
	back in evaluate_singlestep.py:
	rollout function is in rollout.py.
	rollout.py:29 determines input to model: episode[]
	env.step() executes one step in simulator
	action comes from model
	env.step gives obs, current_info
	get_task_info_for_set: returns true/false for success
	original code: evaluate_policy_singlestep.py - bug-free rollout.

2023-03-29  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* warning:
	/userfiles/dyuret/.julia/conda/3/x86_64/lib/python3.10/site-packages/torch/nn/modules/transformer.py:544: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/aten/src/ATen/native/transformers/attention.cpp:150.)

	* gpt.py: Experiments on the D dataset.

	### robot: state,action=>next-state ###
	tcp+0,act+0=>tcp+1: 4921796: min rmsd=0.0422, converges to < 0.05 in 500 steps. {'batch_size': 32, 'max_steps': 100000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'data': 'D'}{'in_features': [('tcp', 0), ('act', 0)], 'out_features': [('tcp', 1)], 'window': 1024}
	  here min-rmsd for xyz = 0.0009, for abc=0.06
	tcp+0,rel+0=>tcp+1: 4921799: min rmsd=0.4470 with simple scaling, converges to 0.35 in 2000 steps. {'batch_size': 32, 'max_steps': 100000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'data': 'D'}{'in_features': [('tcp', 0), ('rel', 0)], 'out_features': [('tcp', 1)], 'window': 1024}
	  here min-rmsd for xyz = 0.0009, for abc=0.6321: we are definitely suffering from angle conversion, **try sin/cos?** try xyz only?
	tcp3+0,act3+0=>tcp3+1: 4921800: min rmsd=0.0009, converges to 0.02286 in 300 steps, 0.00375 in 7000. {'batch_size': 32, 'max_steps': 100000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'data': 'D'}{'in_features': [('tcp3', 0), ('act3', 0)], 'out_features': [('tcp3', 1)], 'window': 1024}
	  here converged to larger loss than min-rmsd: try hparams?
	tcp3+0,rel3+0=>tcp3+1: 4921801: min rmsd=0.0009, converges to 0.006 in 5000 steps.  {'batch_size': 32, 'max_steps': 100000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'data': 'D'}{'in_features': [('tcp3', 0), ('rel3', 0)], 'out_features': [('tcp3', 1)], 'window': 1024}
	  looks good.
	arm+0,act3+0=>arm+1: 4921829: converges to 0.014 in 1800, <0.01 in 4500, 0.00681 in 10K. {'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'data': 'D'}{'in_features': [('arm', 0), ('act3', 0)], 'out_features': [('arm', 1)], 'window': 1024}
	  looks good.
	arm+0,rel3+0=>arm+1: 4921830: converges to 0.013 in 1100, <0.01 in 3700, 0.00621 in 10K. {'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'data': 'D'}{'in_features': [('arm', 0), ('rel3', 0)], 'out_features': [('arm', 1)], 'window': 1024}
	  there is some unstability during training around 800 steps, try gradient clipping?
	arm+0,rel3+0=>arm+1: 4921835: {'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0.5, 'data': 'D'}{'in_features': [('arm', 0), ('rel3', 0)], 'out_features': [('arm', 1)], 'window': 1024}
	  same experiment as above with gradient_clip_val=0.5. does not seem to help much, training curve more noisy, maybe 0.5 not good? lower learning_rate?
	arm+0,rel3+0=>arm+1: 4921836:
	  same experiment as above with no compilation. same speed.

	### robot: state,next-state => action ###
	tcp3+0,tcp3+1=>act3+0: 4921847: trn=0.0067 val=0.0046. {'in_features': [('tcp3', 0), ('tcp3', 1)], 'out_features': [('act3', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	tcp3+0,tcp3+1=>rel3+0: 4921848: trn=0.015 val=0.017. {'in_features': [('tcp3', 0), ('tcp3', 1)], 'out_features': [('rel3', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	arm+0,arm+1=>rel3+0: 4921850: trn=0.021 val=0.025. {'in_features': [('arm', 0), ('arm', 1)], 'out_features': [('rel3', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	tcp3+0,tcp3+16=>rel3+0: 4921851: trn=0.038 val=0.04. {'in_features': [('tcp3', 0), ('tcp3', 16)], 'out_features': [('rel3', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	arm+0,arm+16=>rel3+0: 4921852: trn=0.04 val=0.05. 10K-steps. still improving. {'in_features': [('arm', 0), ('arm', 16)], 'out_features': [('rel3', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	  they did not converge yet, try longer run.

	### scene: state,action=>next-state ###
	tcp0,scn0,act0=>scn1: 4922109: overfitting, trnloss>0.1 {'in_features': [('tcp', 0), ('scene', 0), ('act', 0)], 'out_features': [('scene', 1)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	tcp0,scn0,rel0=>scn1: 4922112: overfitting, trnloss>0.1 {'in_features': [('tcp', 0), ('scene', 0), ('rel', 0)], 'out_features': [('scene', 1)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}

	### scene: state,next-state => action ###
	tcp0,scn0,scn1=>act0: 4922116: overfitting, trnloss=0.1 {'in_features': [('tcp', 0), ('scene', 0), ('scene', 1)], 'out_features': [('act', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	  this is hard to guess, scene does not always change.
	tcp0,scn0,scn1=>rel0: 4922117: val=0.11 {'in_features': [('tcp', 0), ('scene', 0), ('scene', 1)], 'out_features': [('rel', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	tcp0,scn0,scn16=>rel0: 4922118: val=0.11 {'in_features': [('tcp', 0), ('scene', 0), ('scene', 16)], 'out_features': [('rel', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	tcp0,scn0,scn32=>rel0: 4922119: val=0.11 {'in_features': [('tcp', 0), ('scene', 0), ('scene', 32)], 'out_features': [('rel', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	tcp0,scn0,scn64=>rel0: 4922120: val=0.11 {'in_features': [('tcp', 0), ('scene', 0), ('scene', 64)], 'out_features': [('rel', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	  32 and 64 look very similar, try 128, 256.
	tcp0,scn0=>rel0: 4922179: val=0.1064 {'in_features': [('tcp', 0), ('scene', 0)], 'out_features': [('rel', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	  this as a baseline without the goal scene. maybe we are just learning to continue the recent action without paying attn to goal.
	tcp3.0,scn0,scn1=>rel3.0: 4922126: val=0.081
	tcp3.0,scn0,scn16=>rel3.0: 4922127: val=0.084
	tcp3.0,scn0,scn32=>rel3.0: 4922128: val=0.085
	tcp3.0,scn0,scn64=>rel3.0: 4922129: val=0.087
	tcp3.0,scn0=>rel3.0: 4922180: val=0.0761 {'in_features': [('tcp3', 0), ('scene', 0)], 'out_features': [('rel3', 0)], 'window': 1024}{'batch_size': 32, 'max_steps': 10000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0, 'weight_decay': 0, 'learning_rate': 0.0001, 'gradient_clip_val': 0, 'data': 'D'}
	  this is also as baseline, guessing the action without a goal. better than the alternatives, so not learning anything from goal.

2023-03-26  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* calvin_dataset.py: Added PlayDataset for self-supervised experiments.

2023-03-20  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* pooling-experiments: Tried max/mean/last pooling for Transformer and LSTM:
	- Transformer-D: max > mean+1% > last+2%, c32 > c64 for max (.6%), c64 > c32 (.3%) for others
	- Transformer-ABCD: mean > last = max (1%), c32 > c64 for max (.5%), c64 > c32 (.6%) for others.
	- Transformer-ABC: did not converge.
	- LSTM-D: max > last+.2% > mean+2.5%. max64>max32 (1.5%), last64>last32 (.1%), mean32>mean64 (4%)
	- LSTM-ABCD: max > last+.1% > mean+.5%. max64>max32 (.5%), last32>last64 (1.5%), mean64>mean32 (.5%)
	- LSTM-ABC: did not converge. mean64 least bad (.8645).

2023-03-19  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* TODO
	- transformer: try max pooling and cls
	- lstm: also can try max and mean pooling instead of the last step (instead of output_interval).
	-- output_interval not the same as instances_per_episode, may explain job_1 LSTM: either add it back, or find optima with output_interval=
	- transformer: look at the effect of positional encoding: does it help?
	- transformer: add nonlinearity at the input? at the output?
	- transformer: does the *sqrt(ndim) scaling at the input help?
	- calvindataset: integrate 2frame into calvindataset as an option: in fact do a general stride argument
	- calvindataset: create an option that will compute and use differences of n frames as features
	- calvindataset: create an option that will take a subset of instances for learning curve
	- Learning curve experiments.
	- SSL experiments
	- Auto-label experiments
	- finish mlp feature exp with abcd
	- rnn and transformer feature exp
	- deprecate loaddata.py
	- Inputs: try different representations (voxels & convolution?) (3-D positional encoding?) (read Palm-E)

	* Leaderboard: 
	job_id	val_acc	step	data	model	batch	lr	maxstp	hidden	layers	dropout	wdecay	nheads	ipe	context	features
	4903594	0.9740	258405	D	TRM	32	0.0001	300000	128	1	0	1	4	1	64	range(0,97)
	4906858	0.9664	284648	D	LSTM	32	0.0001	300000	256	2	0.5	0.25	0	1	64	range(0,97)
	4903683	0.9570	278208	D	MLP	32	0.0001	300000	512	2	0.75	0.15	0	1	64	range(0,97)
	4906862	0.9604	276430	ABCD	TRM	32	0.0001	300000	128	3	0.75	0.5	1	1	32	range(0,97)
	4906851	0.9565	292842	ABCD	MLP	32	0.0001	300000	512	2	0.5	0.25	0	8	32	range(0,97)
	4903627	0.9560	269874	ABCD	LSTM	32	0.0001	300000	512	4	0.25	0.1	0	8	64	range(0,97)
	4904912	0.9380	283413	ABC	TRM	32	0.00001	300000	512	2	0.95	0.1	1	1	64	range(0,97)
	4906875	0.9367	294855	ABC	MLP	32	0.00001	300000	512	2	0.5	1	0	16	64	range(0,97)
***	4903612	0.8510	142960	ABC	LSTM	32	0.0001	300000	256	3	0.05	1.5	0	32	64	range(0,97)

	- some results have high (>1%) stdev
	- new lstm abc still not converging without output_interval (with output_interval=32 .9011) try mean/max pooling)?

2023-03-18  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* calvin_supervised_slurm.py:
	+ print out number/id of finished jobs.
	+ save their output as each job finishes.
	+ shorten job name, have both slurm id and job row number.
	+ when cancelled due to time limit it doesn't have the result, should eval outside.
	++ also necessary because internal eval may not match if instances_per_episode > 1.
	+ also save the step number of the optimum model.
	+ sometimes saves multiple checkpoints? interrupt-restarts? check all.
	+ figure out the major differences in ABC, and minor ones in ABCD.
	++ wnorm different, but expected, changed the calculation
	++ learning_rate parameter not passed correctly: may explain job_0, Transformer

	* TODO:
	+ transformer
	+ transformer hparam opt
	+ transformer: figure out the ABC hparams: decrease learning rate?
	+ do a sensitivity to layers, heads, hidden etc. for each sota model

2023-03-17  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* Transformer-hparams:
	4900191	TRMABC	.9278	{'model': 'Transformer', 'batch_size': 32, 'lr': 1e-05, 'max_steps': 300000, 'hidden_size': 512, 'num_layers': 2, 'num_heads': 2, 'dropout': 0.95, 'weight_decay': 0.1}{'instances_per_episode': 1, 'context_length': 64, 'features': range(0, 97)}
	4899497	TRMABCD	.95054	{'model': 'Transformer', 'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden_size': 128, 'num_layers': 3, 'num_heads': 2, 'dropout': 0.75, 'weight_decay': 0.5}{'instances_per_episode': 1, 'context_length': 64, 'features': range(0, 97)}
	4899466	TRMD	.9634	{'model': 'Transformer', 'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden_size': 256, 'num_layers': 1, 'num_heads': 4, 'dropout': 0, 'weight_decay': 1.0}{'instances_per_episode': 1, 'context_length': 64, 'features': range(0, 97)}

	* Leaderboard:
	RNNABC	.9011	lightning_logs/version_4880581/checkpoints/epoch=128-step=72111.ckpt 	{'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden_size': 256, 'num_layers': 3, 'output_interval': 32, 'dropout': 0.1, 'weight_decay': 1.5}{'instances_per_episode': 1, 'context_length': 64, 'features': range(0, 97)}
	MLPABC	.9139	lightning_logs/version_4880570/checkpoints/epoch=20-step=93828.ckpt	{'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden': [512, 512], 'dropout': 0.5, 'weight_decay': 1.0}{'instances_per_episode': 8, 'context_length': 64, 'features': range(0, 97)}
	RNNABCD	.9496	lightning_logs/version_4866737/checkpoints/epoch=48-step=35182.ckpt	{'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden_size': 512, 'num_layers': 4, 'output_interval': 8, 'dropout': 0, 'weight_decay': 0.1}{'instances_per_episode': 1, 'context_length': 64, 'features': range(0, 97)}
	MLPABCD	.9476	lightning_logs/version_4867417/checkpoints/epoch=15-step=91872.ckpt	{'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden': [512, 512], 'dropout': 0.5, 'weight_decay': 0.25}{'instances_per_episode': 8, 'context_length': 64, 'features': range(0, 97)}
	RNND	.9604	lightning_logs/version_4858151/checkpoints/epoch=458-step=73899.ckpt 	{'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden_size': 256, 'num_layers': 2, 'output_interval': 1, 'dropout': 0.4, 'weight_decay': 0.25}{'instances_per_episode': 1, 'context_length': 64, 'features': range(0, 97)}
	MLPD	.9525	lightning_logs/version_4881657/checkpoints/epoch=412-step=66493.ckpt	{'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden': [512, 512], 'dropout': 0.6, 'weight_decay': 0.15}{'instances_per_episode': 1, 'context_length': 64, 'features': range(0, 97)}


2023-03-16  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* Transformers:
	- Karpathy: implements from scratch, may not be efficient: https://github.com/karpathy/nanoGPT,miniGPT
	- Pytorch-examples: https://github.com/pytorch/examples/tree/main/word_language_model
	- Huggingface: https://github.com/huggingface/transformers.git
	- Huggingface/Accelerator: https://github.com/huggingface/accelerate (alternative to Lightning?)
	- Huggingface/Optimum: https://github.com/huggingface/optimum (quantization and pruning)
	- https://github.com/huggingface/notebooks: educational
	- https://github.com/huggingface/course: educational
	- https://github.com/huggingface/llm_training_handbook: educational

	* SequenceClassifier: can we write one class that encapsulates mlp, rnn, transformer:
	Input is (B,T,X) sequence.
	Output is (B,C) logits.
	No more output_interval for rnns. Probably no instances_per_episode?
	Either use input_size, hidden_size, num_classes and num_layers for all or use single sizes array.
	mlp uses Sequential. transformer uses ModuleList.
	Difference? ModuleList does not chain the modules automatically but implements parameters, to_device etc.
	rnn currently has two parts for lstm and projector. lstm has multiple outputs, which is incompatible with Sequential.
	Sequential needs one input one output in each module.
	However see this: https://stackoverflow.com/questions/44130851/simple-lstm-in-pytorch-with-sequential-module
	We can use this to implement pos_encoder as well.
	word_language_model also keeps modules in separate fields.
	seems could have used Sequential except for masking which we don't need and is an optional parameter.
	transformer also has extra hyperparameters of nhead and dim_feedforward.

	* word_language_model: https://github.com/pytorch/examples/tree/main/word_language_model
	python -mpdb main.py --cuda --epochs 6 --model Transformer --lr 5
	defaults:
	--emsize=200
	--nhid=200
	--nlayers=2
	--lr=20 (what kind of optimizer?) => seems simple SGD with huge LR?
	--clip=0.25
	--epochs=40
	--batch_size=20
	--bptt=35 (sequence length)
	--dropout=0.2
	--nhead=2
	--tied=not used in transformers
	ntokens = len(corpus.dictionary)

	TransformerModel(
	ntoken=ntokens,	;33278
	ninp=args.emsize,	;200
	nhead=args.nhead,	;2
	nhid=args.nhid,	;200
	nlayers=args.nlayers,	;2
	dropout=args.dropout).to(device)

	TransformerEncoderLayer(
	d_model = ninp = 200
	nhead = nhead = 2
	dim_feedforward = nhid = 200
	dropout = 0.2	; the rest is defaults
	activation = relu
	layer_norm_eps = 1e-5
	batch_first = False
	norm_first = False)

	TransformerEncoderLayer.forward(src, src_mask=None, src_key_padding_mask=None, is_causal=False)
	TransformerEncoder(encoder_layer, num_layers, norm=None, enable_nested_tensor=True, mask_check=True)
	? src_mask vs src_key_padding_mask
	? is_causal
	? order of ops in forward
	? norm=None
	? enable_nested_tensor?
	? mask_check?

	self.ninp=200
	nlayers=2
	self.encoder=Embedding(ntoken=33278,ninp=200)
	self.decoder=Linear(ninp=200->ntoken=33278)
	nhead=2
	nhid=200
	dropout=0.2

	forward(src=T:35,B:20,int64 has_mask=True)
	mask=35,35,float32 with 0 lower-left, -inf upper-right triangular (diagonal=0)
	src = self.encoder(src=35,20,int64) * math.sqrt(self.ninp=200)
	=> src=35,20,200,float32 (T,B,H)
	src = self.pos_encoder(src)
	=> src=35,20,200,float32
	output = self.transformer_encoder(src, self.src_mask)
	=> output=35,20,200,float32
	output = self.decoder(output)
	=> output=35,20,33278,float32
	return F.log_softmax(output, dim=-1)
	=> return=35,20,33278,float32, normalized in vocab dimension

	This predicts all words, using masking for training. How does it generate?
	input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device) ; initialization to random token
        output = model(input, False) ; mask=false
        word_weights = output[-1].squeeze().div(args.temperature).exp().cpu()
        word_idx = torch.multinomial(word_weights, 1)[0]
        word_tensor = torch.Tensor([[word_idx]]).long().to(device)
        input = torch.cat([input, word_tensor], 0)

	For classification chatgpt recommends cls token or max/mean pooling.
	Emre Can also added using all output tokens (concatenate) since we have fixed length vectors.
	Or using encoder-decoder (decoder would only produce a single token, which would be eq to cls?)


2023-03-15  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* mlp-d-features.py: Yesterday's two scene frame experiments gave confusing results:
	- On ABC they got .88, close to full-feature .91 showing two scene frames enough.
	- On D they got .70, significantly lower than full-feature .95.
	- On D using multiple frames instead of two with scene features also gave .70.
	= To understand what is going on: do some feature ablation experiments on D.
	- Analyze the confusion matrix for D.

	sizes,dropout,weight_decay,lr,hp_metric
	"(4672, 512, 512, 34)",0.5,0.1,0.0001,0.9584569931030273 excl=scn
	"(5952, 512, 512, 34)",0.5,0.1,0.0001,0.9545004963874817 excl=grp
	"(5824, 512, 512, 34)",0.5,0.1,0.0001,0.9515331387519836 excl=act
	"(5440, 512, 512, 34)",0.5,0.1,0.0001,0.950543999671936  excl=cont
	"(5696, 512, 512, 34)",0.5,0.1,0.0001,0.950543999671936  excl=tact
	"(5824, 512, 512, 34)",0.5,0.1,0.0001,0.9495549201965332 excl=tcp
	"(5824, 512, 512, 34)",0.5,0.1,0.0001,0.9485657811164856 excl=ract
	"(6208, 512, 512, 34)",0.5,0.1,0.0001,0.9485657811164856 excl=none
	"(5760, 512, 512, 34)",0.5,0.1,0.0001,0.947576642036438  excl=arm
	"(1536, 512, 512, 34)",0.5,0.1,0.0001,0.9208704233169556 incl=sdiff
	"(4672, 512, 512, 34)",0.5,0.1,0.0001,0.9179030656814575 excl=sdiff
	"(1536, 512, 512, 34)",0.5,0.1,0.0001,0.7200791239738464 incl=scn
	"(448, 512, 512, 34)",0.5,0.1,0.0001,0.5806132555007935  incl=arm
	"(384, 512, 512, 34)",0.5,0.1,0.0001,0.5410484671592712  incl=tcp
	"(384, 512, 512, 34)",0.5,0.1,0.0001,0.5370919704437256  incl=act
	"(384, 512, 512, 34)",0.5,0.1,0.0001,0.5341246128082275  incl=ract
	"(256, 512, 512, 34)",0.5,0.1,0.0001,0.2502472698688507  incl=grp
	"(768, 512, 512, 34)",0.5,0.1,0.0001,0.24431255459785461 incl=cont
	"(512, 512, 512, 34)",0.5,0.1,0.0001,0.21661721169948578 incl=tact

	- sdiff seems most important, scn least? how can it not learn to derive sdiff from scn?
	- scn only is second best so maybe it does. but the diff is huge: .92 vs .72
	- let's look at include2 and exclude2

	sizes,dropout,weight_decay,lr,hp_metric
	"(4288, 512, 512, 34)",0.5,0.1,0.0001,0.9614243507385254 333 excl=scn+tcp
	"(4416, 512, 512, 34)",0.5,0.1,0.0001,0.9594460725784302 330 excl=scn+grp
	"(3904, 512, 512, 34)",0.5,0.1,0.0001,0.9584569931030273 336 excl=scn+cont
	"(4288, 512, 512, 34)",0.5,0.1,0.0001,0.9574678540229797 332 excl=scn+ract
	"(4672, 512, 512, 34)",0.5,0.1,0.0001,0.9574678540229797 335 excl=scn
	"(4672, 512, 512, 34)",0.5,0.1,0.0001,0.9574678540229797 339 excl=scn
	"(4288, 512, 512, 34)",0.5,0.1,0.0001,0.9564787149429321 331 excl=scn+act
	"(4224, 512, 512, 34)",0.5,0.1,0.0001,0.9554896354675293 334 excl=scn+arm
	"(4160, 512, 512, 34)",0.5,0.1,0.0001,0.9545004963874817 337 excl=scn+tact
	"(1984, 512, 512, 34)",0.5,0.1,0.0001,0.9446092844009399 323 incl=sdiff+arm
	"(1920, 512, 512, 34)",0.5,0.1,0.0001,0.9426310658454895 322 incl=sdiff+tcp
	"(1920, 512, 512, 34)",0.5,0.1,0.0001,0.9416419267654419 320 incl=sdiff+act
	"(1920, 512, 512, 34)",0.5,0.1,0.0001,0.9406528472900391 321 incl=sdiff+ract
	"(4416, 512, 512, 34)",0.5,0.1,0.0001,0.9357072114944458 340 excl=sdiff+grp
	"(2048, 512, 512, 34)",0.5,0.1,0.0001,0.9337289929389954 326 incl=sdiff+tact
	"(1792, 512, 512, 34)",0.5,0.1,0.0001,0.9277942776679993 319 incl=sdiff+grp
	"(3072, 512, 512, 34)",0.5,0.1,0.0001,0.9218595623970032 291 incl=scn+sdiff
	"(1536, 512, 512, 34)",0.5,0.1,0.0001,0.9188922047615051 328 incl=sdiff
	"(3904, 512, 512, 34)",0.5,0.1,0.0001,0.9188922047615051 346 excl=sdiff+cont
	"(2304, 512, 512, 34)",0.5,0.1,0.0001,0.9179030656814575 325 incl=sdiff+cont
	"(1536, 512, 512, 34)",0.5,0.1,0.0001,0.9169139266014099 327 incl=sdiff
	"(4288, 512, 512, 34)",0.5,0.1,0.0001,0.9169139266014099 341 excl=sdiff+act
	"(4288, 512, 512, 34)",0.5,0.1,0.0001,0.9169139266014099 342 excl=sdiff+ract
	"(4288, 512, 512, 34)",0.5,0.1,0.0001,0.9159248471260071 343 excl=sdiff+tcp
	"(3072, 512, 512, 34)",0.5,0.1,0.0001,0.912957489490509  324 incl=sdiff+scn
	"(4160, 512, 512, 34)",0.5,0.1,0.0001,0.912957489490509  347 excl=sdiff+tact
	"(4672, 512, 512, 34)",0.5,0.1,0.0001,0.912957489490509  349 excl=sdiff
	"(4672, 512, 512, 34)",0.5,0.1,0.0001,0.909990131855011  350 excl=sdiff
	"(4224, 512, 512, 34)",0.5,0.1,0.0001,0.9060336351394653 344 excl=sdiff+arm
	"(1984, 512, 512, 34)",0.5,0.1,0.0001,0.8704253435134888 287 incl=scn+arm
	"(1920, 512, 512, 34)",0.5,0.1,0.0001,0.860534131526947  285 incl=scn+ract
	"(1920, 512, 512, 34)",0.5,0.1,0.0001,0.8417408466339111 283 incl=scn+act
	"(1920, 512, 512, 34)",0.5,0.1,0.0001,0.8397626280784607 286 incl=scn+tcp
	"(2048, 512, 512, 34)",0.5,0.1,0.0001,0.7952522039413452 290 incl=scn+tact
	"(1792, 512, 512, 34)",0.5,0.1,0.0001,0.7764589786529541 282 incl=scn+grp
	"(1536, 512, 512, 34)",0.5,0.1,0.0001,0.727992057800293  288 incl=scn
	"(1536, 512, 512, 34)",0.5,0.1,0.0001,0.7250247001647949 292 incl=scn
	"(2304, 512, 512, 34)",0.5,0.1,0.0001,0.7161226272583008 289 incl=scn+cont
	"(3136, 512, 512, 34)",0.5,0.1,0.0001,0.6152324676513672 345 excl=sdiff+scn
	"(3136, 512, 512, 34)",0.5,0.1,0.0001,0.6083086133003235 338 excl=scn+sdiff

	- Obviously scn and sdiff are the most important, if both are missing we go .90->.60.
	- sdiff more useful than scn, having both is unnecessary and redundant?
	- excluding scn+tact (act,arm,tact) gives the biggest drop in scn+ group, tact is the least redundant with sdiff?
	- excluding scn+tcp (grp,cont,ract) gives even better result than scn alone.
	- excluding sdiff+arm gives the biggest drop in sdiff+ group, arm is the least redundant scn?
	- including sdiff+(arm,tcp,act,ract) gives the biggest boost. .94+
	- including sdiff+(tact,grp,cont) gives a bit less, cont almost none.
	- (sdiff,scn) is the scene group, sdiff is more useful, why?.
	- (arm,tcp,act,ract) is the robot group, arm is most useful, slightly more than others, why?.
	- (tact,cont,grp) is the rest, tact seems most useful.

	* mlp-abc: let's repeat the same experiments with abc:

	- Very different results:
	- scn seems a lot more important than sdiff.
	- having them both seems to be useful, incl scn+sdiff is best.
	- excluding act has the biggest impact after excluding scn. excluding sdiff seems to improve!

	sizes,dropout,weight_decay,lr,hp_metric
	"(5440, 512, 512, 34)",0.5,1.0,0.0001,0.9264342188835144 500 excl=cont
	"(4672, 512, 512, 34)",0.5,1.0,0.0001,0.9117210507392883 502 excl=sdiff
	"(6208, 512, 512, 34)",0.5,1.0,0.0001,0.8916913866996765 503 excl=none
	"(5696, 512, 512, 34)",0.5,1.0,0.0001,0.8814292550086975 501 excl=tact
	"(3072, 512, 512, 34)",0.5,1.0,0.0001,0.8592977523803711 488 incl=sdiff+scn
	"(3072, 512, 512, 34)",0.5,1.0,0.0001,0.8569485545158386 481 incl=scn+sdiff
	"(5760, 512, 512, 34)",0.5,1.0,0.0001,0.854846715927124  498 excl=arm
	"(5824, 512, 512, 34)",0.5,1.0,0.0001,0.8503956198692322 496 excl=ract
	"(5824, 512, 512, 34)",0.5,1.0,0.0001,0.850271999835968  497 excl=tcp
	"(5952, 512, 512, 34)",0.5,1.0,0.0001,0.8486647009849548 494 excl=grp
	"(1920, 512, 512, 34)",0.5,1.0,0.0001,0.8482937812805176 476 incl=scn+tcp
	"(1920, 512, 512, 34)",0.5,1.0,0.0001,0.8461918830871582 474 incl=scn+act
	"(5824, 512, 512, 34)",0.5,1.0,0.0001,0.8360534310340881 495 excl=act
	"(1984, 512, 512, 34)",0.5,1.0,0.0001,0.830118715763092  477 incl=scn+arm
	"(1920, 512, 512, 34)",0.5,1.0,0.0001,0.8293768763542175 475 incl=scn+ract
	"(1984, 512, 512, 34)",0.5,1.0,0.0001,0.8084816932678223 487 incl=sdiff+arm
	"(1536, 512, 512, 34)",0.5,1.0,0.0001,0.7915430068969727 469 incl=sdiff
	"(1536, 512, 512, 34)",0.5,1.0,0.0001,0.7884520292282104 491 incl=sdiff
	"(1920, 512, 512, 34)",0.5,1.0,0.0001,0.7882047295570374 485 incl=sdiff+ract
	"(2048, 512, 512, 34)",0.5,1.0,0.0001,0.7844955325126648 490 incl=sdiff+tact
	"(1536, 512, 512, 34)",0.5,1.0,0.0001,0.7757171392440796 493 incl=sdiff
	"(1920, 512, 512, 34)",0.5,1.0,0.0001,0.7737388610839844 484 incl=sdiff+act
	"(1792, 512, 512, 34)",0.5,1.0,0.0001,0.7707715034484863 483 incl=sdiff+grp
	"(1920, 512, 512, 34)",0.5,1.0,0.0001,0.7611275911331177 486 incl=sdiff+tcp
	"(4672, 512, 512, 34)",0.5,1.0,0.0001,0.738254189491272  499 excl=scn
	"(2048, 512, 512, 34)",0.5,1.0,0.0001,0.7381305694580078 480 incl=scn+tact
	"(1792, 512, 512, 34)",0.5,1.0,0.0001,0.7345449924468994 473 incl=scn+grp
	"(1536, 512, 512, 34)",0.5,1.0,0.0001,0.6709940433502197 478 incl=scn
	"(1536, 512, 512, 34)",0.5,1.0,0.0001,0.6687685251235962 466 incl=scn
	"(1536, 512, 512, 34)",0.5,1.0,0.0001,0.659866452217102  482 incl=scn
	"(2304, 512, 512, 34)",0.5,1.0,0.0001,0.6543026566505432 479 incl=scn+cont
	"(2304, 512, 512, 34)",0.5,1.0,0.0001,0.5395647883415222 489 incl=sdiff+cont
	"(384, 512, 512, 34)",0.5,1.0,0.0001,0.47205737233161926 462 incl=ract
	"(384, 512, 512, 34)",0.5,1.0,0.0001,0.4540059268474579  464 incl=tcp
	"(384, 512, 512, 34)",0.5,1.0,0.0001,0.4485657811164856  461 incl=act
	"(448, 512, 512, 34)",0.5,1.0,0.0001,0.4483185112476349  465 incl=arm
	"(512, 512, 512, 34)",0.5,1.0,0.0001,0.25                468 incl=tact
	"(768, 512, 512, 34)",0.5,1.0,0.0001,0.20697329938411713 467 incl=cont
	"(256, 512, 512, 34)",0.5,1.0,0.0001,0.16765578091144562 460 incl=grp


2023-03-14  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* TwoFrameDataset:
	MLPABC2	.8803	lightning_logs/version_4881634/checkpoints/epoch=170-step=95589.ckpt -f'range(29,53)' -c32 -f'range(29,53)'	{'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden': [512, 512], 'dropout': 0.25, 'weight_decay': 0}{'context_length': 32, 'features': range(29, 53)}
	MLPD2	.7074?	4881674? mlp-d6 overfitting? two frames {'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden': [512, 512], 'dropout': 0.75, 'weight_decay': 0.5}{'context_length': 48, 'features': range(29, 53)}
	MLPD2x	.7062	4881870? mlp-d8 cover same range but all frames drop0.25, wdec0.1 {'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden': [512, 512], 'dropout': 0.25, 'weight_decay': 0.1}{'context_length': 32, 'features': range(29, 53)}
	MLPD2x  .6597   4881877 mlp-d9  same as d8 drop0.5 wdec1.0 {'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden': [512, 512], 'dropout': 0.5, 'weight_decay': 1.0}{'context_length': 32, 'features': range(29, 53)}
	MLPABC2x .7626  4881872? mlp-abc4 cover same range but all frames {'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden': [512, 512], 'dropout': 0.5, 'weight_decay': 1.0}


2023-03-12  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* TODO:
	+ publish corrected data.
	+ are annotations the same for D, ABC and ABCD for corresponding frames? No. only ABC-validation=ABCD-validation.
	+ convert data loader to npz compressed.
	+ eval.py not working with calvindataset.py?
	+ look at ABC trn D val confusion matrix what it mostly gets wrong.
	x make validation subsets out of A,B,C as well and see if ABC model can do well with those.
	+ do feature ablation experiments to see what features it relies on.
	+ Inputs: try different subsets

	* Leaderboard: After fixing the color-swap bug (x marks runs with buggy dataset for comparison):
	RNNABC	.8912	lightning_logs/version_4853865/checkpoints/epoch=116-step=65403.ckpt	{'batch_size': 32, 'hidden_size': 256, 'num_layers': 2, 'output_interval': 32, 'lr': 0.0001, 'max_steps': 100000, 'dropout': 0.1, 'weight_decay': 2.0}
	RNNABCx	.8645	lightning_logs/version_4852828/checkpoints/epoch=177-step=99502.ckpt -f'range(73,97)'	{'batch_size': 32, 'hidden_size': 128, 'num_layers': 1, 'output_interval': 16, 'lr': 0.0001, 'max_steps': 100000, 'dropout': 0.75, 'weight_decay': 1.0}{'instances_per_episode': 1, 'context_length': 64, 'features': range(73, 97)}
	MLPABC	.8863	mlp_project/glfqykq5/checkpoints/epoch=10-step=49148.ckpt	{'batch_size': 32, 'hidden': [256], 'lr': 0.0001, 'max_steps': 100000, 'dropout': 0.75, 'weight_decay': 0.5}{'instances_per_episode': 8, 'context_length': 64, 'features': range(0, 97)}
	MLPABCx	.7250	mlp_project/scsbwf6q/checkpoints/epoch=266-step=149253.ckpt -c 64 -f 'list(range(0,53))+list(range(65,73))'	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'dropout': 0.75, 'weight_decay': 0.25}; instances_per_episode=1, context_length=64, features=list(range(0,53))+list(range(65,73))
	RNNABCD	.9486	lightning_logs/version_4854077/checkpoints/epoch=91-step=66056.ckpt	{'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden_size': 512, 'num_layers': 4, 'output_interval': 4, 'dropout': 0.1, 'weight_decay': 0.05}
	RABCDx	.9169	lightning_logs/version_4852496/checkpoints/epoch=291-step=209656.ckpt 	{'batch_size': 32, 'hidden_size': 512, 'num_layers': 3, 'output_interval': 1, 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.25, 'weight_decay': 0.1}
	MLPABCD	.9466	lightning_logs/version_4854088/checkpoints/epoch=15-step=91872.ckpt -c64	{'batch_size': 32, 'lr': 0.0001, 'max_steps': 100000, 'hidden': [128, 128], 'dropout': 0.25, 'weight_decay': 0}{'instances_per_episode': 8, 'context_length': 64, 'features': range(0, 97)}
	MABCDx	.9397	mlp_project/p1rzqxgm/checkpoints/epoch=317-step=228324.ckpt -c 32	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.5, 'weight_decay': 0.1} (instances_per_episode=1, context_length=32)
	RNND	.9495	lightning_logs/version_4852815/checkpoints/epoch=306-step=49427.ckpt -f'range(73,97)'	{'batch_size': 32, 'hidden_size': 512, 'num_layers': 2, 'output_interval': 1, 'lr': 0.0001, 'max_steps': 50000, 'dropout': 0, 'weight_decay': 0.1}{'instances_per_episode': 1, 'context_length': 64, 'features': range(73, 97)}
	MLPD	.9446	mlp_project/0xftsmbg/checkpoints/epoch=927-step=149408.ckpt -c 32	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.5, 'weight_decay': 0.1}

	* Experiments: After fixing the color-swap bug try:
	- rnn-abc-sdiff3.py: rnn-abc with only sdiff features: .8329
	- rnn-abcfixed(2).py: rnn-abc with all features: .8912
	- rnn-abcfixed-scene+sdiff.py: rnn-abc with scene and sdiff features. .8645
	- mlp-abcfixed.py: mlp-abc with all features.

	* calvin_scene_check.py: bugs:
	ABC/training:{'calvin_scene_B': [0, 598909], 'calvin_scene_C': [598910, 1191338], 'calvin_scene_A': [1191339, 1795044]} |
	113221	rotate_red_block_left	mindist=blue	maxstd=red (isolated bad annotation)
	599905-1191237	red-blue switch in calvin_scene_C (all 2216 instances containing (red|blue))
	1192931-1794376 pink-red switch in calvin_scene_A (all 2455 instances containing (red|pink))

	ABCD/training | {'calvin_scene_D': [0, 611098], 'calvin_scene_B': [611099, 1210008], 'calvin_scene_C': [1210009, 1802437], 'calvin_scene_A': [1802438, 2406143]}
	449310	rotate_blue_block_left	mindist=pink	maxstd=pink (isolated bad annotation)
	538891	lift_pink_block_slider	mindist=blue	maxstd=pink (ok)
	724320	rotate_red_block_left	mindist=blue	maxstd=red  (same as ABC/113221)
	724325	rotate_red_block_left	mindist=blue	maxstd=blue (same as ABC/113221)
	782940	rotate_pink_block_right	mindist=red	maxstd=pink (isolated bad annotation)
	1210994-1802343	red-blue switch in scene C (all 2323 instances)
	1802564-2405710 red-pink switch in scene A (2366/2367 instances)

	All other files (debug, D, and all validation) are ok.

	* Annotation-comparison:
	ABC-validation == ABCD-validation
	ABC-validation != D-validation
	D-training != D-subset of ABCD-training
	ABC-training != ABC-subset of ABCD-training

2023-03-11  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* TODO:
	+ validate rel_actions formula
	+ Inputs: Emphasize the differences using a smart scaled difference functions (careful about episode boundaries and 2pi angle differences).
	+ run experiments with scene diff only, should be enough?
	+ Print out confusion matrix, esp for ABC.
	+ some classes need absolute position, try scene + scene_diff: did not improve on RNND.

	* leaderboard: using the new sdiff features with rnns improved some of them:

	RNNABC	.8645	lightning_logs/version_4852828/checkpoints/epoch=177-step=99502.ckpt -f'range(74,98)'	{'batch_size': 32, 'hidden_size': 128, 'num_layers': 1, 'output_interval': 16, 'lr': 0.0001, 'max_steps': 100000, 'dropout': 0.75, 'weight_decay': 1.0}{'instances_per_episode': 1, 'context_length': 64, 'features': range(74, 98)}
	MLPABC	.7250	mlp_project/scsbwf6q/checkpoints/epoch=266-step=149253.ckpt -c 64 -f 'list(range(1,54))+list(range(66,74))'	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'dropout': 0.75, 'weight_decay': 0.25}; instances_per_episode=1, context_length=64, features=list(range(1,54))+list(range(66,74))
	RNNABCD	.9169	lightning_logs/version_4852496/checkpoints/epoch=291-step=209656.ckpt 	{'batch_size': 32, 'hidden_size': 512, 'num_layers': 3, 'output_interval': 1, 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.25, 'weight_decay': 0.1}
	MLPABCD	.9397	mlp_project/p1rzqxgm/checkpoints/epoch=317-step=228324.ckpt -c 32	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.5, 'weight_decay': 0.1} (instances_per_episode=1, context_length=32)
	RNND	.9495	lightning_logs/version_4852815/checkpoints/epoch=306-step=49427.ckpt -f'range(74,98)'	{'batch_size': 32, 'hidden_size': 512, 'num_layers': 2, 'output_interval': 1, 'lr': 0.0001, 'max_steps': 50000, 'dropout': 0, 'weight_decay': 0.1}{'instances_per_episode': 1, 'context_length': 64, 'features': range(74, 98)}
	MLPD	.9446	mlp_project/0xftsmbg/checkpoints/epoch=927-step=149408.ckpt -c 32	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.5, 'weight_decay': 0.1}

	* bugs:
	ABC-training: 1511823	1511887	rotate_pink_block_left	take the pink block and turn it left (scene A)
	The annotation is consistent with the visual. But there is no pink motion recorded in scene_obs.
	The motion consistent with tcp seems to be in the red coordinates.


2023-03-10  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* TODO:
	x see if additional annotations improve ABC: data is fully annotated!

	* leaderboard: Re-evaluate using D/validation and predicting only the last frame of every episode.

	RNNABC	.7349	lightning_logs/version_4852401/checkpoints/epoch=349-step=195650.ckpt	{'batch_size': 32, 'hidden_size': 256, 'num_layers': 1, 'output_interval': 1, 'lr': 0.0001, 'max_steps': 400000, 'dropout': 0.75, 'weight_decay': 1.5}(instances_per_episode=1, context_length=64)
	MLPABC	.7250	mlp_project/scsbwf6q/checkpoints/epoch=266-step=149253.ckpt -c 64 -f 'list(range(1,54))+list(range(66,74))'	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'dropout': 0.75, 'weight_decay': 0.25}; instances_per_episode=1, context_length=64, features=list(range(1,54))+list(range(66,74))
	RNNABCD	.9169	lightning_logs/version_4852496/checkpoints/epoch=291-step=209656.ckpt 	{'batch_size': 32, 'hidden_size': 512, 'num_layers': 3, 'output_interval': 1, 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.25, 'weight_decay': 0.1}
	MLPABCD	.9397	mlp_project/p1rzqxgm/checkpoints/epoch=317-step=228324.ckpt -c 32	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.5, 'weight_decay': 0.1} (instances_per_episode=1, context_length=32)
	RNND	.9001	lightning_logs/version_4852748/checkpoints/epoch=516-step=83237.ckpt	{'batch_size': 32, 'hidden_size': 128, 'num_layers': 2, 'output_interval': 1, 'lr': 0.0001, 'max_steps': 100000, 'dropout': 0.5, 'weight_decay': 2.0}
	MLPD	.9446	mlp_project/0xftsmbg/checkpoints/epoch=927-step=149408.ckpt -c 32	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.5, 'weight_decay': 0.1}

2023-03-09  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* 16-action-frames-to-train: The system predicts 'turn_on_led'
	when the robot just hovers over the button but does not push
	it. Reducing the training signal from the last 32 frames of each
	annotation to 16 seems better from visualizing a couple of
	examples. Otherwise a lot of "waiting in front of something"
	frames get tagged as having accomplished the action. Restricting
	to a single frame per episode is also possible but: (1) reduces
	the amount of training data, (2) episode ends are not precise,
	sometimes the action (e.g. turning on the light) happens a bit
	earlier.

	* rnn-abcd-best:
	{'batch_size': 128, 'hidden_size': 512, 'num_layers': 3, 'output_interval': 32, 'lr': 0.001, 'max_steps': 150000, 'dropout': 0.25, 'weight_decay': 0.1}
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)
	m1 = rnn.LitRNN.load_from_checkpoint('lightning_logs/version_4851386/checkpoints/epoch=433-step=78120.ckpt')
	abcval = ld.CalvinDataset("../data/ABC-validation", instances_per_episode=32, context_length=64)
	dval = ld.CalvinDataset("../data/D-validation", instances_per_episode=32, context_length=64)
	# using ipe=32 is not ideal, instead check the val_acc_n for ipe=1.
	abcval1 = ld.CalvinDataset("../data/ABC-validation", instances_per_episode=1, context_length=64)
	dval1 = ld.CalvinDataset("../data/D-validation", instances_per_episode=1, context_length=64)
	tr.validate(m1, DataLoader(abcval1, batch_size=128)) #=> [{'hp_metric': 0.9954001903533936, 'val_loss_all': 1.1064249277114868, 'val_acc_all': 0.7174131870269775, 'val_loss_1': 0.021554984152317047, 'val_acc_1': 0.9954001903533936, 'val_loss_n': 0.07080072909593582, 'val_acc_n': 0.976914644241333}]
	tr.validate(m1, DataLoader(abcval, batch_size=128))  #=> [{'hp_metric': 0.9954001903533936, 'val_loss_all': 2.9308674335479736, 'val_acc_all': 0.45334240794181824, 'val_loss_1': 1.4030355215072632, 'val_acc_1': 0.7879197597503662, 'val_loss_n': 2.2739408016204834, 'val_acc_n': 0.6895744204521179}]
	tr.validate(m1, DataLoader(dval1, batch_size=128))   #=> [{'hp_metric': 0.8921859264373779, 'val_loss_all': 1.781881332397461, 'val_acc_all': 0.6030075550079346, 'val_loss_1': 0.64299476146698, 'val_acc_1': 0.8921859264373779, 'val_loss_n': 0.8299209475517273, 'val_acc_n': 0.8604723215103149}]
	tr.validate(m1, DataLoader(dval, batch_size=128))    #=> [{'hp_metric': 0.8921859264373779, 'val_loss_all': 3.8475852012634277, 'val_acc_all': 0.353348046541214, 'val_loss_1': 2.4296398162841797, 'val_acc_1': 0.6806379556655884, 'val_loss_n': 3.5552220344543457, 'val_acc_n': 0.557407557964325}]

	trn		abcval	abcval1	dval	dval1
	---		------	-------	----	-----
	RNNABCD-32x64	.9769	.9954	.8605	.8922
	MLPABCD-32x32	.9577	.9604	.8935	.9070

	* rnn-abc-best:
	{'batch_size': 128, 'hidden_size': 512, 'num_layers': 3, 'output_interval': 32, 'lr': 0.001, 'max_steps': 10000, 'dropout': 0.5, 'weight_decay': 1.5}
	m2 = rnn.LitRNN.load_from_checkpoint('lightning_logs/version_4851451/checkpoints/epoch=50-step=7140.ckpt')
	tr.validate(m2, DataLoader(abcval1, batch_size=128)) #=> [{'hp_metric': 0.7580496668815613, 'val_loss_all': 1.585593342781067, 'val_acc_all': 0.5129513740539551, 'val_loss_1': 0.49294838309288025, 'val_acc_1': 0.7580496668815613, 'val_loss_n': 0.6073561310768127, 'val_acc_n': 0.7320894598960876}]
	tr.validate(m2, DataLoader(dval1, batch_size=128)) #=> [{'hp_metric': 0.7580496668815613, 'val_loss_all': 2.0052120685577393, 'val_acc_all': 0.4292933940887451, 'val_loss_1': 0.782939076423645, 'val_acc_1': 0.7082096934318542, 'val_loss_n': 0.9975055456161499, 'val_acc_n': 0.6633592844009399}]

	trn		abcval	abcval1	dval	dval1
	---		------	-------	----	-----
	RNNABC-32x64	.7321	.7580	.6634	.7082
	MLPABC-32x32	.6136	.5832	.5854	.5658

2023-03-08  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* rnn: ideas
	- hyperparameter optimize: hidden, dropout, weight_decay, lr
	- play with output_interval
	- play with sampling frequency (skip every other frame etc)
	- represent scaled differences as features
	- take a look at the last frame performance for validation

	* TODO:
	+ try rnn
	+ try tensorboard


2023-03-07  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* MLP-Summary:
	trn		abcval	abcval1	dval	dval1
	---		------	-------	----	-----
	ABCD-32x32	.9577	.9604	.8935	.9070
	ABCD-1x32	.5306	.9890	.4319	.9199
	ABCD-32x4	.9113	.8418	.8810	.8299
	ABCD-32x1	.8711	.7939	.8378	.7893
	ABC-32x32	.6136	.5832	.5854	.5658
	ABC-1x32	.3329	.7121	.2848	.6469
	ABC-32x4	.6403	.5915	.6159	.5697
	ABC-32x1	.6380	.6109	.6060	.5687
	D-32x32		.8968	.8905	.8827	.8892
	D-1x32   	.5507	.9154	.4623	.9426
	D-32x4   	.8969	.8151	.8817	.8051
	D-32x1   	.8764	.7884	.8667	.7784

	Notes:
	- ABC-32x4 means 32 instances per episode from its last 32 frames, each instance has a 4 frame context.
	- abcval evaluates accuracy on the last 32 frames of each episode, abcval1 evaluates only on the last frame.
	- ABCD and ABC used earlystop based on abcval, D used earlystop based on dval.
	- D contains training instances only from D, ABC from A,B,C and ABCD is a union of the two training sets.
	- The validation sets contain instances only from D (disjoint from training).
	- abcval and abcdval has the same language annotations, dval has different language annotations.

	* TODO:
	+ Train a 32x32 model.

2023-03-06  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* +: See how episode-based (ABCD) model does on an individual frame basis.
	>>> for i in [1,2,4,8,16,32]:
	...   d = ld.calvindataset("../data/ABC-validation", instances_per_episode=i, context_length=32)
	...   print('instances per episode = ', i)
	...   tr.validate(m1, DataLoader(d, batch_size=32))
	last-1-frame:   [{'val_loss': 0.046598080545663834, 'val_acc': 0.9889604449272156}]
	last-2-frames:  [{'val_loss': 0.04988009110093117,  'val_acc': 0.9885004758834839}]
	last-4-frames:  [{'val_loss': 0.0645752102136612,   'val_acc': 0.9855105876922607}]
	last-8-frames:  [{'val_loss': 0.14425814151763916,  'val_acc': 0.9643514156341553}]
	last-16-frames: [{'val_loss': 0.7874042987823486,   'val_acc': 0.8522309064865112}]
	last-32-frames: [{'val_loss': 4.200976848602295,    'val_acc': 0.5305888056755066}]
	last-32-frames: [{'val_loss': 0.4216434359550476,   'val_acc': 0.871147632598877}]   (32x1-frame-model)
	last-32-frames: [{'val_loss': 0.24731987714767456,  'val_acc': 0.9218031167984009}]  (32x4-frame-model)

	* +: See how frame-based (ABCD) model does on the last frame of each episode.
	>>> d = ld.calvindataset("../data/ABC-validation", instances_per_episode=1, context_length=(4|1))
	32x1-frame-model: [{'val_loss': 0.6967383623123169, 'val_acc': 0.7939282655715942}]  (on last frame)
	32x1-frame-model: [{'val_loss': 0.4216434359550476, 'val_acc': 0.871147632598877}]   (on all frames)
	32x4-frame-model: [{'val_loss': 0.6505945324897766, 'val_acc': 0.8012879490852356}]  (on last frame)
	32x4-frame-model: [{'val_loss': 0.2473198771476745, 'val_acc': 0.9218031167984009}]  (on all frames)
	32x32-frame-model:
	32x32-frame-model:

	* +: See what per-annotation accuracy is using majority voting of 32x1-frame or 32x4-frame models.


2023-03-05  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* === MLP-Experiments-1x32frame: context_length=32, instances_per_episode=1

	* ABCD-1x32: wandb/run-20230305_224324-fqm3f8nl: (y1zidphb:dark-sky-84)
	- Can get 98% val acc with (using last 32 frames and all 73 features):
	- Note the large difference between abcval and dval (same data different annotations).
	abcdtrn = ld.calvindataset("../data/ABCD-training", instances_per_episode=1, context_length=32)
	abcdval = ld.calvindataset("../data/ABCD-validation", instances_per_episode=1, context_length=32))
	mlp.train(abcdtrn, abcdval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/y1zidphb/checkpoints/epoch=180-step=129958.ckpt')
	tr.validate(m1, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.046598080545663834, 'val_acc': 0.9889604449272156}]
	tr.validate(m1, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.4002395272254944, 'val_acc': 0.919881284236908}]

	* ABC-1x32: wandb/run-20230306_001846-3xaj8o6q: (e95afymd:drawn-wood-83)
	- Only able to get to %70 even with strong regularization:
	abctrn = ld.calvindataset("../data/ABC-training", instances_per_episode=1, context_length=32)
	abcval = ld.calvindataset("../data/ABC-validation", instances_per_episode=1, context_length=32)
	mlp.train(abctrn, abcval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.7, weight_decay=0.2, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/e95afymd/checkpoints/epoch=77-step=43602.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.6437634825706482, 'val_acc': 0.712051510810852}]
	tr.validate(m2, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 1.0606377124786377, 'val_acc': 0.6468842625617981}]

	* D-1x32: wandb/run-20230306_001635-p06ygu9d: (0xftsmbg:eternal-grass-82)
	- Pretty much all experiments converge to 93% for a range of regularizations.
	ABCD does significantly better on the same validation set.
	But D language annotations are different! And this time the order is reversed? Cooked val set for D-training?
	dtrn = ld.calvindataset("../data/ABC-training", instances_per_episode=1, context_length=32)
	dval = ld.calvindataset("../data/ABC-validation", instances_per_episode=1, context_length=32)
	mlp.train(dtrn, dval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/0xftsmbg/checkpoints/epoch=927-step=149408.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.3091912567615509, 'val_acc': 0.9153633713722229}]
	tr.validate(m3, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.25575390458106995, 'val_acc': 0.9426310658454895}]


	* === MLP-Experiments-32x1-frame: context_length=1, instances_per_episode=32
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)

	* ABCD-32x1:
	abcdtrn = ld.calvindataset("../data/ABCD-training", instances_per_episode=32, context_length=1)
	abcdval = ld.calvindataset("../data/ABCD-validation", instances_per_episode=32, context_length=1)
	dval = ld.calvindataset1("../data/D-validation")
	mlp.train(abcdtrn, abcdval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/iyyed7m1/checkpoints/epoch=5-step=137796.ckpt')
	tr.validate(m1, DataLoader(abcdval, batch_size=32)) #=> [{'val_loss': 0.4216434359550476, 'val_acc': 0.871147632598877}]
	tr.validate(m1, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 0.6235239505767822, 'val_acc': 0.8378152847290039}]

	* ABC-32x1:
	abctrn = ld.calvindataset("../data/ABC-training", instances_per_episode=32, context_length=1)
	abcval = ld.calvindataset("../data/ABC-validation", instances_per_episode=32, context_length=1)
	mlp.train(abctrn, abcval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/ondcx26m/checkpoints/epoch=5-step=107220.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.9127740859985352, 'val_acc': 0.6380232572555542}]
	tr.validate(m2, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 1.1524760723114014, 'val_acc': 0.6060212850570679}]

	* D-32x1:
	dtrn = ld.calvindataset("../data/D-training", instances_per_episode=32, context_length=1)
	dval = ld.calvindataset("../data/D-validation", instances_per_episode=32, context_length=1)
	mlp.train(dtrn, dval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/ltfpotls/checkpoints/epoch=27-step=143472.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.38017138838768005, 'val_acc': 0.8763511776924133}]
	tr.validate(m3, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.4305930435657501,  'val_acc': 0.8666542768478394}]


	* === MLP-Experiments-32x4-frame: context_length=4, instances_per_episode=32
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)

	* ABCD-32x4:
	abcdtrn = ld.calvindataset("../data/ABCD-training", instances_per_episode=32, context_length=4)
	abcdval = ld.calvindataset("../data/ABCD-validation", instances_per_episode=32, context_length=4)
	dval = ld.calvindataset("../data/D-validation", instances_per_episode=32, context_length=4)
	mlp.train(abcdtrn, abcdval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/dv9032kv/checkpoints/epoch=5-step=137796.ckpt')
	tr.validate(m1, DataLoader(abcdval, batch_size=32)) #=> [{'val_loss': 0.2637326419353485, 'val_acc': 0.9113097786903381}]
	tr.validate(m1, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 0.4974829852581024, 'val_acc': 0.8810274600982666}]

	* ABC-32x4:
	abctrn = ld.calvindataset("../data/ABC-training", instances_per_episode=32, context_length=4)
	abcval = ld.calvindataset("../data/ABC-validation", instances_per_episode=32, context_length=4)
	mlp.train(abctrn, abcval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/0heo6gfj/checkpoints/epoch=2-step=53610.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.9169828295707703, 'val_acc': 0.6402943730354309}]
	tr.validate(m2, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 1.178528904914856, 'val_acc': 0.6159433722496033}]

	* D-32x4:
	dtrn = ld.calvindataset("../data/D-training", instances_per_episode=32, context_length=4)
	dval = ld.calvindataset("../data/D-validation", instances_per_episode=32, context_length=4)
	mlp.train(dtrn, dval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/w29sr8tr/checkpoints/epoch=23-step=122976.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.31888464093208313, 'val_acc': 0.8968778848648071}]
	tr.validate(m3, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.38751137256622314, 'val_acc': 0.8817383646965027}]


	* === MLP-Experiments-32x32-frame: context_length=32, instances_per_episode=32
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)
	dval = ld.calvindataset("../data/D-validation", instances_per_episode=32, context_length=32)
	dval1 = ld.calvindataset("../data/D-validation", instances_per_episode=1, context_length=32)
	abcval = ld.calvindataset("../data/ABC-validation", instances_per_episode=32, context_length=32)
	abcval1 = ld.calvindataset("../data/ABC-validation", instances_per_episode=1, context_length=32)

	* ABCD-32x32:
	abcdtrn = ld.calvindataset("../data/ABCD-training", instances_per_episode=32, context_length=32)
	mlp.train(abcdtrn, abcval, max_steps=1500000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/i7t7yfv6/checkpoints/epoch=16-step=390422.ckpt')
	tr.validate(m1, DataLoader(abcval, batch_size=32))  #=> [{'val_loss': 0.12283673137426376, 'val_acc': 0.9576817154884338}]
	tr.validate(m1, DataLoader(abcval1, batch_size=32)) #=> [{'val_loss': 0.1442822366952896, 'val_acc': 0.9604415893554688}]
	tr.validate(m1, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 0.5026547908782959, 'val_acc': 0.8935151100158691}]
	tr.validate(m1, DataLoader(dval1, batch_size=32))   #=> [{'val_loss': 0.3591073453426361, 'val_acc': 0.9070227742195129}]

	* ABC-32x32: this is too low, more regularization needed
	abctrn = ld.calvindataset("../data/ABC-training", instances_per_episode=32, context_length=32)
	mlp.train(abctrn, abcval, max_steps=1500000, batch_size=32, lr=0.0001, dropout=0.75, weight_decay=0.25, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/0dhghbx7/checkpoints/epoch=5-step=107220.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32))  #=> [{'val_loss': 0.9245778322219849, 'val_acc': 0.6136441826820374}]
	tr.validate(m2, DataLoader(abcval1, batch_size=32)) #=> [{'val_loss': 0.9777761101722717, 'val_acc': 0.5832566618919373}]
	tr.validate(m2, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 1.2631582021713257, 'val_acc': 0.5854042768478394}]
	tr.validate(m2, DataLoader(dval1, batch_size=32))   #=> [{'val_loss': 1.1301568746566772, 'val_acc': 0.5657764673233032}]

	* D-32x32:
	dtrn = ld.calvindataset("../data/D-training", instances_per_episode=32, context_length=32)
	mlp.train(dtrn, dval, max_steps=1500000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/bv1t44ac/checkpoints/epoch=41-step=215208.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32))  #=> [{'val_loss': 0.3558889627456665, 'val_acc': 0.8967629075050354}]
	tr.validate(m3, DataLoader(abcval1, batch_size=32)) #=> [{'val_loss': 0.3719893097877502, 'val_acc': 0.8905243873596191}]
	tr.validate(m3, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 0.3902618885040283, 'val_acc': 0.8826656937599182}]
	tr.validate(m3, DataLoader(dval1, batch_size=32))   #=> [{'val_loss': 0.4161476790904999, 'val_acc': 0.8892185688018799}]


	* bugs:
	+ the validation results do not match when reload from checkpoint!
	We need to call m.eval() before accuracy. Call m.train() again to activate dropout etc.
	+ the validation sets of D and ABC are different (same data different language. ABC=ABCD)
	Reporting both
	+ wandb: cannot see max validation accuracy!
	pytorch lightning: checkpoint model at best val: https://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing_intermediate.html
	use checkpoint_callback = ModelCheckpoint(monitor = "val_acc", mode = 'max')
	pl.Trainer(..., callbacks=[checkpoint_callback], ...)


	* hyperparameters:
	>>> a = torch.load('mlp_project/v3vfpaws/checkpoints/epoch=178-step=100000.ckpt')
	>>> a["hyper_parameters"]
	{'sizes': (2336, 512, 512, 34), 'lr': 0.0001, 'weight_decay': 0.1, 'dropout': 0.6}


2023-03-04  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* mlp:
	+ 512 hidden 0.5 dropout gives 82% validation accuracy on D for single frame using all features.
	+ Using 2 or 3 frames only increase this less than 1%.
	+ Visualize the predictions to see what is going on.

	* system:
	+ error in process filter: No such directory found via CDPATH environment variable (emacs terminal with srun)
	  fixed disabling term-command-hook.

	* pl:
	+ forward vs predict_step: do we need both? forward is needed for predict.
	+ wandb init: can we log every run? currently starting python every time: need to call wandb.finish() at the end of experiment.


2023-03-02  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* wandb: Emre Can notes:
	Accuracy Metric code-line: https://github.com/emrecanacikgoz/robot-language/blob/2520cef611b115eb62938dc8859be0738f0a4946/blind_robot/models/mlp.py#L82
	training epoch ending function: https://github.com/emrecanacikgoz/robot-language/blob/2520cef611b115eb62938dc8859be0738f0a4946/blind_robot/models/mlp.py#L88
	my wandb logger: https://github.com/emrecanacikgoz/robot-language/blob/2520cef611b115eb62938dc8859be0738f0a4946/main.py#L93
	project arguman yeni proje ayor hocam
	name arguman ilgili projenin iindeki run ad
	ayn proje iin several run yapcaksanz name'i deitirebilirsiniz

	* TODO:
	+ load checkpoint
	+ measure val/trn loss
	+ measure val/trn acc
	+ wandb: when do you get a new run?
	+ visualize training curves
	+ normalization
	x overfitting: usual methods did not work, try feature selection, try delta of scene coordinates, delta of robot obs, lower frame rate, multiple frames.


2023-02-23  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* TODO:
	+ extract tactile pixels
	+ see if controller coordinates are in the simulator: https://github.com/mees/calvin_env

2023-02-22  dyuret  <dyuret@login02.kuacc.ku.edu.tr>

	* controller-coordinates: door, drawer, button, switch.


2023-02-19  Deniz Yuret  <dyuret@WS001>

	* scene_info.npy:
	# | debug/training | {'calvin_scene_D': [358482, 361252]} |
	# | debug/validation | {'calvin_scene_D': [553567, 555241]} |
	# | D/training | {'calvin_scene_A': [0, 611098]} |
	# | D/validation | . |
	# | ABC/training | {'calvin_scene_B': [0, 598909], 'calvin_scene_C': [598910, 1191338], 'calvin_scene_A': [1191339, 1795044]} |
	# | ABC/validation | . |
	# | ABCD/training | {'calvin_scene_A': [1802438, 2406143], 'calvin_scene_B': [611099, 1210008], 'calvin_scene_C': [1210009, 1802437], 'calvin_scene_D': [0, 611098]} |
	# | ABCD/validation | . |


	* TODO:
	+ write intervals.py to check intervals: done, several off-by-one errors in ABCD/training (frames common with validation).
	+ check overlaps see if identical: all identical but renumbered.
	+ check visual difference in A vs D: B is lightest, A is darkest, C and D similar with C having a straight pattern, D a bit lighter.
	+ see if ep_start_end is consistent with frame diffs
	+ improved visualizer to see what number is what
