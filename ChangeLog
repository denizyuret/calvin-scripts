2023-03-10  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* TODO:
	- make validation subsets out of A,B,C as well and see if ABC model can do well with those.
	- look at ABC trn D val confusion matrix what it mostly gets wrong.
	- do feature ablation experiments to see what features it relies on.

	* leaderboard: Re-evaluate using D/validation and predicting only the last frame of every episode.

	RNNABC	.7349	lightning_logs/version_4852401/checkpoints/epoch=349-step=195650.ckpt	{'batch_size': 32, 'hidden_size': 256, 'num_layers': 1, 'output_interval': 1, 'lr': 0.0001, 'max_steps': 400000, 'dropout': 0.75, 'weight_decay': 1.5}(instances_per_episode=1, context_length=64)
	MLPABC	.7250	mlp_project/scsbwf6q/checkpoints/epoch=266-step=149253.ckpt -c 64 -f 'list(range(1,54))+list(range(66,74))'	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_epochs': 300, 'dropout': 0.75, 'weight_decay': 0.25}; instances_per_episode=1, context_length=64, features=list(range(1,54))+list(range(66,74))
	RNNABCD	.9169	lightning_logs/version_4852496/checkpoints/epoch=291-step=209656.ckpt 	{'batch_size': 32, 'hidden_size': 512, 'num_layers': 3, 'output_interval': 1, 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.25, 'weight_decay': 0.1}
	MLPABCD	.9397	mlp_project/p1rzqxgm/checkpoints/epoch=317-step=228324.ckpt -c 32	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.5, 'weight_decay': 0.1} (instances_per_episode=1, context_length=32)
	RNND	.8358	lightning_logs/version_4852497/checkpoints/epoch=1545-step=248906.ckpt	{'batch_size': 32, 'hidden_size': 512, 'num_layers': 3, 'output_interval': 1, 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.25, 'weight_decay': 0.1}
	MLPD	.9446	mlp_project/0xftsmbg/checkpoints/epoch=927-step=149408.ckpt -c 32	{'batch_size': 32, 'hidden': [512, 512], 'lr': 0.0001, 'max_steps': 300000, 'dropout': 0.5, 'weight_decay': 0.1}

2023-03-09  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* 16-action-frames-to-train: The system predicts 'turn_on_led'
	when the robot just hovers over the button but does not push
	it. Reducing the training signal from the last 32 frames of each
	annotation to 16 seems better from visualizing a couple of
	examples. Otherwise a lot of "waiting in front of something"
	frames get tagged as having accomplished the action. Restricting
	to a single frame per episode is also possible but: (1) reduces
	the amount of training data, (2) episode ends are not precise,
	sometimes the action (e.g. turning on the light) happens a bit
	earlier.

	* rnn-abcd-best:
	{'batch_size': 128, 'hidden_size': 512, 'num_layers': 3, 'output_interval': 32, 'lr': 0.001, 'max_steps': 150000, 'dropout': 0.25, 'weight_decay': 0.1}
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)
	m1 = rnn.LitRNN.load_from_checkpoint('lightning_logs/version_4851386/checkpoints/epoch=433-step=78120.ckpt')
	abcval = ld.CalvinDataset("../data/ABC-validation", instances_per_episode=32, context_length=64)
	dval = ld.CalvinDataset("../data/D-validation", instances_per_episode=32, context_length=64)
	# using ipe=32 is not ideal, instead check the val_acc_n for ipe=1.
	abcval1 = ld.CalvinDataset("../data/ABC-validation", instances_per_episode=1, context_length=64)
	dval1 = ld.CalvinDataset("../data/D-validation", instances_per_episode=1, context_length=64)
	tr.validate(m1, DataLoader(abcval1, batch_size=128)) #=> [{'hp_metric': 0.9954001903533936, 'val_loss_all': 1.1064249277114868, 'val_acc_all': 0.7174131870269775, 'val_loss_1': 0.021554984152317047, 'val_acc_1': 0.9954001903533936, 'val_loss_n': 0.07080072909593582, 'val_acc_n': 0.976914644241333}]
	tr.validate(m1, DataLoader(abcval, batch_size=128))  #=> [{'hp_metric': 0.9954001903533936, 'val_loss_all': 2.9308674335479736, 'val_acc_all': 0.45334240794181824, 'val_loss_1': 1.4030355215072632, 'val_acc_1': 0.7879197597503662, 'val_loss_n': 2.2739408016204834, 'val_acc_n': 0.6895744204521179}]
	tr.validate(m1, DataLoader(dval1, batch_size=128))   #=> [{'hp_metric': 0.8921859264373779, 'val_loss_all': 1.781881332397461, 'val_acc_all': 0.6030075550079346, 'val_loss_1': 0.64299476146698, 'val_acc_1': 0.8921859264373779, 'val_loss_n': 0.8299209475517273, 'val_acc_n': 0.8604723215103149}]
	tr.validate(m1, DataLoader(dval, batch_size=128))    #=> [{'hp_metric': 0.8921859264373779, 'val_loss_all': 3.8475852012634277, 'val_acc_all': 0.353348046541214, 'val_loss_1': 2.4296398162841797, 'val_acc_1': 0.6806379556655884, 'val_loss_n': 3.5552220344543457, 'val_acc_n': 0.557407557964325}]

	trn		abcval	abcval1	dval	dval1
	---		------	-------	----	-----
	RNNABCD-32x64	.9769	.9954	.8605	.8922
	MLPABCD-32x32	.9577	.9604	.8935	.9070

	* rnn-abc-best:
	{'batch_size': 128, 'hidden_size': 512, 'num_layers': 3, 'output_interval': 32, 'lr': 0.001, 'max_steps': 10000, 'dropout': 0.5, 'weight_decay': 1.5}
	m2 = rnn.LitRNN.load_from_checkpoint('lightning_logs/version_4851451/checkpoints/epoch=50-step=7140.ckpt')
	tr.validate(m2, DataLoader(abcval1, batch_size=128)) #=> [{'hp_metric': 0.7580496668815613, 'val_loss_all': 1.585593342781067, 'val_acc_all': 0.5129513740539551, 'val_loss_1': 0.49294838309288025, 'val_acc_1': 0.7580496668815613, 'val_loss_n': 0.6073561310768127, 'val_acc_n': 0.7320894598960876}]
	tr.validate(m2, DataLoader(dval1, batch_size=128)) #=> [{'hp_metric': 0.7580496668815613, 'val_loss_all': 2.0052120685577393, 'val_acc_all': 0.4292933940887451, 'val_loss_1': 0.782939076423645, 'val_acc_1': 0.7082096934318542, 'val_loss_n': 0.9975055456161499, 'val_acc_n': 0.6633592844009399}]

	trn		abcval	abcval1	dval	dval1
	---		------	-------	----	-----
	RNNABC-32x64	.7321	.7580	.6634	.7082
	MLPABC-32x32	.6136	.5832	.5854	.5658

2023-03-08  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* rnn: ideas
	- hyperparameter optimize: hidden, dropout, weight_decay, lr
	- play with output_interval
	- play with sampling frequency (skip every other frame etc)
	- represent scaled differences as features
	- take a look at the last frame performance for validation

	* TODO:
	+ try rnn
	+ try tensorboard
	- Print out confusion matrix, esp for ABC.
	- see if additional annotations improve ABC
	- Inputs: Emphasize the differences using a smart scaled difference functions (careful about episode boundaries and 2pi angle differences).
	- Inputs: try different subsets
	- Inputs: try different representations (voxels & convolution?)
	- validate rel_actions formula


2023-03-07  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* MLP-Summary:
	trn		abcval	abcval1	dval	dval1
	---		------	-------	----	-----
	ABCD-32x32	.9577	.9604	.8935	.9070
	ABCD-1x32	.5306	.9890	.4319	.9199
	ABCD-32x4	.9113	.8418	.8810	.8299
	ABCD-32x1	.8711	.7939	.8378	.7893
	ABC-32x32	.6136	.5832	.5854	.5658
	ABC-1x32	.3329	.7121	.2848	.6469
	ABC-32x4	.6403	.5915	.6159	.5697
	ABC-32x1	.6380	.6109	.6060	.5687
	D-32x32		.8968	.8905	.8827	.8892
	D-1x32   	.5507	.9154	.4623	.9426
	D-32x4   	.8969	.8151	.8817	.8051
	D-32x1   	.8764	.7884	.8667	.7784

	Notes:
	- ABC-32x4 means 32 instances per episode from its last 32 frames, each instance has a 4 frame context.
	- abcval evaluates accuracy on the last 32 frames of each episode, abcval1 evaluates only on the last frame.
	- ABCD and ABC used earlystop based on abcval, D used earlystop based on dval.
	- D contains training instances only from D, ABC from A,B,C and ABCD is a union of the two training sets.
	- The validation sets contain instances only from D (disjoint from training).
	- abcval and abcdval has the same language annotations, dval has different language annotations.

	* TODO:
	+ Train a 32x32 model.

2023-03-06  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* +: See how episode-based (ABCD) model does on an individual frame basis.
	>>> for i in [1,2,4,8,16,32]:
	...   d = ld.calvindataset("../data/ABC-validation", instances_per_episode=i, context_length=32)
	...   print('instances per episode = ', i)
	...   tr.validate(m1, DataLoader(d, batch_size=32))
	last-1-frame:   [{'val_loss': 0.046598080545663834, 'val_acc': 0.9889604449272156}]
	last-2-frames:  [{'val_loss': 0.04988009110093117,  'val_acc': 0.9885004758834839}]
	last-4-frames:  [{'val_loss': 0.0645752102136612,   'val_acc': 0.9855105876922607}]
	last-8-frames:  [{'val_loss': 0.14425814151763916,  'val_acc': 0.9643514156341553}]
	last-16-frames: [{'val_loss': 0.7874042987823486,   'val_acc': 0.8522309064865112}]
	last-32-frames: [{'val_loss': 4.200976848602295,    'val_acc': 0.5305888056755066}]
	last-32-frames: [{'val_loss': 0.4216434359550476,   'val_acc': 0.871147632598877}]   (32x1-frame-model)
	last-32-frames: [{'val_loss': 0.24731987714767456,  'val_acc': 0.9218031167984009}]  (32x4-frame-model)

	* +: See how frame-based (ABCD) model does on the last frame of each episode.
	>>> d = ld.calvindataset("../data/ABC-validation", instances_per_episode=1, context_length=(4|1))
	32x1-frame-model: [{'val_loss': 0.6967383623123169, 'val_acc': 0.7939282655715942}]  (on last frame)
	32x1-frame-model: [{'val_loss': 0.4216434359550476, 'val_acc': 0.871147632598877}]   (on all frames)
	32x4-frame-model: [{'val_loss': 0.6505945324897766, 'val_acc': 0.8012879490852356}]  (on last frame)
	32x4-frame-model: [{'val_loss': 0.2473198771476745, 'val_acc': 0.9218031167984009}]  (on all frames)
	32x32-frame-model:
	32x32-frame-model:

	* +: See what per-annotation accuracy is using majority voting of 32x1-frame or 32x4-frame models.


2023-03-05  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* === MLP-Experiments-1x32frame: context_length=32, instances_per_episode=1

	* ABCD-1x32: wandb/run-20230305_224324-fqm3f8nl: (y1zidphb:dark-sky-84)
	- Can get 98% val acc with (using last 32 frames and all 73 features):
	- Note the large difference between abcval and dval (same data different annotations).
	abcdtrn = ld.calvindataset("../data/ABCD-training", instances_per_episode=1, context_length=32)
	abcdval = ld.calvindataset("../data/ABCD-validation", instances_per_episode=1, context_length=32))
	mlp.train(abcdtrn, abcdval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/y1zidphb/checkpoints/epoch=180-step=129958.ckpt')
	tr.validate(m1, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.046598080545663834, 'val_acc': 0.9889604449272156}]
	tr.validate(m1, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.4002395272254944, 'val_acc': 0.919881284236908}]

	* ABC-1x32: wandb/run-20230306_001846-3xaj8o6q: (e95afymd:drawn-wood-83)
	- Only able to get to %70 even with strong regularization:
	abctrn = ld.calvindataset("../data/ABC-training", instances_per_episode=1, context_length=32)
	abcval = ld.calvindataset("../data/ABC-validation", instances_per_episode=1, context_length=32)
	mlp.train(abctrn, abcval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.7, weight_decay=0.2, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/e95afymd/checkpoints/epoch=77-step=43602.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.6437634825706482, 'val_acc': 0.712051510810852}]
	tr.validate(m2, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 1.0606377124786377, 'val_acc': 0.6468842625617981}]

	* D-1x32: wandb/run-20230306_001635-p06ygu9d: (0xftsmbg:eternal-grass-82)
	- Pretty much all experiments converge to 93% for a range of regularizations.
	ABCD does significantly better on the same validation set.
	But D language annotations are different! And this time the order is reversed? Cooked val set for D-training?
	dtrn = ld.calvindataset("../data/ABC-training", instances_per_episode=1, context_length=32)
	dval = ld.calvindataset("../data/ABC-validation", instances_per_episode=1, context_length=32)
	mlp.train(dtrn, dval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/0xftsmbg/checkpoints/epoch=927-step=149408.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.3091912567615509, 'val_acc': 0.9153633713722229}]
	tr.validate(m3, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.25575390458106995, 'val_acc': 0.9426310658454895}]


	* === MLP-Experiments-32x1-frame: context_length=1, instances_per_episode=32
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)

	* ABCD-32x1:
	abcdtrn = ld.calvindataset("../data/ABCD-training", instances_per_episode=32, context_length=1)
	abcdval = ld.calvindataset("../data/ABCD-validation", instances_per_episode=32, context_length=1)
	dval = ld.calvindataset1("../data/D-validation")
	mlp.train(abcdtrn, abcdval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/iyyed7m1/checkpoints/epoch=5-step=137796.ckpt')
	tr.validate(m1, DataLoader(abcdval, batch_size=32)) #=> [{'val_loss': 0.4216434359550476, 'val_acc': 0.871147632598877}]
	tr.validate(m1, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 0.6235239505767822, 'val_acc': 0.8378152847290039}]

	* ABC-32x1:
	abctrn = ld.calvindataset("../data/ABC-training", instances_per_episode=32, context_length=1)
	abcval = ld.calvindataset("../data/ABC-validation", instances_per_episode=32, context_length=1)
	mlp.train(abctrn, abcval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/ondcx26m/checkpoints/epoch=5-step=107220.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.9127740859985352, 'val_acc': 0.6380232572555542}]
	tr.validate(m2, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 1.1524760723114014, 'val_acc': 0.6060212850570679}]

	* D-32x1:
	dtrn = ld.calvindataset("../data/D-training", instances_per_episode=32, context_length=1)
	dval = ld.calvindataset("../data/D-validation", instances_per_episode=32, context_length=1)
	mlp.train(dtrn, dval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/ltfpotls/checkpoints/epoch=27-step=143472.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.38017138838768005, 'val_acc': 0.8763511776924133}]
	tr.validate(m3, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.4305930435657501,  'val_acc': 0.8666542768478394}]


	* === MLP-Experiments-32x4-frame: context_length=4, instances_per_episode=32
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)

	* ABCD-32x4:
	abcdtrn = ld.calvindataset("../data/ABCD-training", instances_per_episode=32, context_length=4)
	abcdval = ld.calvindataset("../data/ABCD-validation", instances_per_episode=32, context_length=4)
	dval = ld.calvindataset("../data/D-validation", instances_per_episode=32, context_length=4)
	mlp.train(abcdtrn, abcdval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/dv9032kv/checkpoints/epoch=5-step=137796.ckpt')
	tr.validate(m1, DataLoader(abcdval, batch_size=32)) #=> [{'val_loss': 0.2637326419353485, 'val_acc': 0.9113097786903381}]
	tr.validate(m1, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 0.4974829852581024, 'val_acc': 0.8810274600982666}]

	* ABC-32x4:
	abctrn = ld.calvindataset("../data/ABC-training", instances_per_episode=32, context_length=4)
	abcval = ld.calvindataset("../data/ABC-validation", instances_per_episode=32, context_length=4)
	mlp.train(abctrn, abcval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/0heo6gfj/checkpoints/epoch=2-step=53610.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.9169828295707703, 'val_acc': 0.6402943730354309}]
	tr.validate(m2, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 1.178528904914856, 'val_acc': 0.6159433722496033}]

	* D-32x4:
	dtrn = ld.calvindataset("../data/D-training", instances_per_episode=32, context_length=4)
	dval = ld.calvindataset("../data/D-validation", instances_per_episode=32, context_length=4)
	mlp.train(dtrn, dval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/w29sr8tr/checkpoints/epoch=23-step=122976.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.31888464093208313, 'val_acc': 0.8968778848648071}]
	tr.validate(m3, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.38751137256622314, 'val_acc': 0.8817383646965027}]


	* === MLP-Experiments-32x32-frame: context_length=32, instances_per_episode=32
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)
	dval = ld.calvindataset("../data/D-validation", instances_per_episode=32, context_length=32)
	dval1 = ld.calvindataset("../data/D-validation", instances_per_episode=1, context_length=32)
	abcval = ld.calvindataset("../data/ABC-validation", instances_per_episode=32, context_length=32)
	abcval1 = ld.calvindataset("../data/ABC-validation", instances_per_episode=1, context_length=32)

	* ABCD-32x32:
	abcdtrn = ld.calvindataset("../data/ABCD-training", instances_per_episode=32, context_length=32)
	mlp.train(abcdtrn, abcval, max_steps=1500000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/i7t7yfv6/checkpoints/epoch=16-step=390422.ckpt')
	tr.validate(m1, DataLoader(abcval, batch_size=32))  #=> [{'val_loss': 0.12283673137426376, 'val_acc': 0.9576817154884338}]
	tr.validate(m1, DataLoader(abcval1, batch_size=32)) #=> [{'val_loss': 0.1442822366952896, 'val_acc': 0.9604415893554688}]
	tr.validate(m1, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 0.5026547908782959, 'val_acc': 0.8935151100158691}]
	tr.validate(m1, DataLoader(dval1, batch_size=32))   #=> [{'val_loss': 0.3591073453426361, 'val_acc': 0.9070227742195129}]

	* ABC-32x32: this is too low, more regularization needed
	abctrn = ld.calvindataset("../data/ABC-training", instances_per_episode=32, context_length=32)
	mlp.train(abctrn, abcval, max_steps=1500000, batch_size=32, lr=0.0001, dropout=0.75, weight_decay=0.25, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/0dhghbx7/checkpoints/epoch=5-step=107220.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32))  #=> [{'val_loss': 0.9245778322219849, 'val_acc': 0.6136441826820374}]
	tr.validate(m2, DataLoader(abcval1, batch_size=32)) #=> [{'val_loss': 0.9777761101722717, 'val_acc': 0.5832566618919373}]
	tr.validate(m2, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 1.2631582021713257, 'val_acc': 0.5854042768478394}]
	tr.validate(m2, DataLoader(dval1, batch_size=32))   #=> [{'val_loss': 1.1301568746566772, 'val_acc': 0.5657764673233032}]

	* D-32x32:
	dtrn = ld.calvindataset("../data/D-training", instances_per_episode=32, context_length=32)
	mlp.train(dtrn, dval, max_steps=1500000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/bv1t44ac/checkpoints/epoch=41-step=215208.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32))  #=> [{'val_loss': 0.3558889627456665, 'val_acc': 0.8967629075050354}]
	tr.validate(m3, DataLoader(abcval1, batch_size=32)) #=> [{'val_loss': 0.3719893097877502, 'val_acc': 0.8905243873596191}]
	tr.validate(m3, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 0.3902618885040283, 'val_acc': 0.8826656937599182}]
	tr.validate(m3, DataLoader(dval1, batch_size=32))   #=> [{'val_loss': 0.4161476790904999, 'val_acc': 0.8892185688018799}]


	* bugs:
	+ the validation results do not match when reload from checkpoint!
	We need to call m.eval() before accuracy. Call m.train() again to activate dropout etc.
	+ the validation sets of D and ABC are different (same data different language. ABC=ABCD)
	Reporting both
	+ wandb: cannot see max validation accuracy!
	pytorch lightning: checkpoint model at best val: https://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing_intermediate.html
	use checkpoint_callback = ModelCheckpoint(monitor = "val_acc", mode = 'max')
	pl.Trainer(..., callbacks=[checkpoint_callback], ...)


	* hyperparameters:
	>>> a = torch.load('mlp_project/v3vfpaws/checkpoints/epoch=178-step=100000.ckpt')
	>>> a["hyper_parameters"]
	{'sizes': (2336, 512, 512, 34), 'lr': 0.0001, 'weight_decay': 0.1, 'dropout': 0.6}


2023-03-04  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* mlp:
	+ 512 hidden 0.5 dropout gives 82% validation accuracy on D for single frame using all features.
	+ Using 2 or 3 frames only increase this less than 1%.
	+ Visualize the predictions to see what is going on.

	* system:
	+ error in process filter: No such directory found via CDPATH environment variable (emacs terminal with srun)
	  fixed disabling term-command-hook.

	* pl:
	+ forward vs predict_step: do we need both? forward is needed for predict.
	+ wandb init: can we log every run? currently starting python every time: need to call wandb.finish() at the end of experiment.


2023-03-02  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* wandb: Emre Can notes:
	Accuracy Metric code-line: https://github.com/emrecanacikgoz/robot-language/blob/2520cef611b115eb62938dc8859be0738f0a4946/blind_robot/models/mlp.py#L82
	training epoch ending function: https://github.com/emrecanacikgoz/robot-language/blob/2520cef611b115eb62938dc8859be0738f0a4946/blind_robot/models/mlp.py#L88
	my wandb logger: https://github.com/emrecanacikgoz/robot-language/blob/2520cef611b115eb62938dc8859be0738f0a4946/main.py#L93
	project argumanı yeni proje açıyor hocam
	name argumanı ilgili projenin içindeki run adı
	aynı proje için several run yapıcaksanız name'i değiştirebilirsiniz

	* TODO:
	+ load checkpoint
	+ measure val/trn loss
	+ measure val/trn acc
	+ wandb: when do you get a new run?
	+ visualize training curves
	+ normalization
	x overfitting: usual methods did not work, try feature selection, try delta of scene coordinates, delta of robot obs, lower frame rate, multiple frames.


2023-02-23  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* TODO:
	+ extract tactile pixels
	+ see if controller coordinates are in the simulator: https://github.com/mees/calvin_env

2023-02-22  dyuret  <dyuret@login02.kuacc.ku.edu.tr>

	* controller-coordinates: door, drawer, button, switch.


2023-02-19  Deniz Yuret  <dyuret@WS001>

	* scene_info.npy:
	# | debug/training | {'calvin_scene_D': [358482, 361252]} |
	# | debug/validation | {'calvin_scene_D': [553567, 555241]} |
	# | D/training | {'calvin_scene_A': [0, 611098]} |
	# | D/validation | . |
	# | ABC/training | {'calvin_scene_B': [0, 598909], 'calvin_scene_C': [598910, 1191338], 'calvin_scene_A': [1191339, 1795044]} |
	# | ABC/validation | . |
	# | ABCD/training | {'calvin_scene_A': [1802438, 2406143], 'calvin_scene_B': [611099, 1210008], 'calvin_scene_C': [1210009, 1802437], 'calvin_scene_D': [0, 611098]} |
	# | ABCD/validation | . |


	* TODO:
	+ write intervals.py to check intervals: done, several off-by-one errors in ABCD/training (frames common with validation).
	+ check overlaps see if identical: all identical but renumbered.
	+ check visual difference in A vs D: B is lightest, A is darkest, C and D similar with C having a straight pattern, D a bit lighter.
	+ see if ep_start_end is consistent with frame diffs
	+ improved visualizer to see what number is what
