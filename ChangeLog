2023-03-06  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* TODO:
	- Train a 32x32 model.
	- Print out confusion matrix.
	- Emphasize the differences using a smart scaled difference functions (careful about episode boundaries and 2pi angle differences).
	- try tensorboard
	- try rnn

	* +: See how episode-based model does on an individual frame basis.
	>>> for i in [1,2,4,8,16,32]:
	...   d = ld.calvindataset("../data/ABC-validation", instances_per_episode=i, context_length=32)
	...   print('instances per episode = ', i)
	...   tr.validate(m1, DataLoader(d, batch_size=32))
	last-1-frame:   [{'val_loss': 0.046598080545663834, 'val_acc': 0.9889604449272156}]
	last-2-frames:  [{'val_loss': 0.04988009110093117,  'val_acc': 0.9885004758834839}]
	last-4-frames:  [{'val_loss': 0.0645752102136612,   'val_acc': 0.9855105876922607}]
	last-8-frames:  [{'val_loss': 0.14425814151763916,  'val_acc': 0.9643514156341553}]
	last-16-frames: [{'val_loss': 0.7874042987823486,   'val_acc': 0.8522309064865112}]
	last-32-frames: [{'val_loss': 4.200976848602295,    'val_acc': 0.5305888056755066}]
	last-32-frames: [{'val_loss': 0.4216434359550476,   'val_acc': 0.871147632598877}]   (32x1-frame-model)
	last-32-frames: [{'val_loss': 0.24731987714767456,  'val_acc': 0.9218031167984009}]  (32x4-frame-model)

	* +: See how frame-based model does on the last frame of each episode.
	>>> d = ld.calvindataset("../data/ABC-validation", instances_per_episode=1, context_length=(4|1))
	32x1-frame-model: [{'val_loss': 0.6967383623123169, 'val_acc': 0.7939282655715942}]  (on last frame)
	32x1-frame-model: [{'val_loss': 0.4216434359550476, 'val_acc': 0.871147632598877}]   (on all frames)
	32x4-frame-model: [{'val_loss': 0.6505945324897766, 'val_acc': 0.8012879490852356}]  (on last frame)
	32x4-frame-model: [{'val_loss': 0.2473198771476745, 'val_acc': 0.9218031167984009}]  (on all frames)

	* +: See what per-annotation accuracy is using majority voting of 32x1-frame or 32x4-frame models.


2023-03-05  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* Summary:
	trn	abcval	dval	exp
	---	------	----	---
	ABCD	.9890	.9199	32-frame
	ABCD	.9218	.8860	4-frame
	ABCD	.8711	.8378	1-frame
	D	.9154	.9426	32-frame
	D	.8982	.8820	4-frame
	D	.8763	.8667	1-frame
	ABC	.7121	.6469	32-frame
	ABC	.6489	.6253	4-frame
	ABC	.6380	.6060	1-frame

	* === MLP-Experiments-1x32frame: context_length=32, instances_per_episode=1

	* ABCD: wandb/run-20230305_224324-fqm3f8nl: (y1zidphb:dark-sky-84)
	- Can get 98% val acc with (using last 32 frames and all 73 features):
	- Note the large difference between abcval and dval (same data different annotations).
	abcdtrn = ld.calvindataset2("../data/ABCD-training")
	abcdval = ld.calvindataset2("../data/ABCD-validation")
	mlp.train(abcdtrn, abcdval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/y1zidphb/checkpoints/epoch=180-step=129958.ckpt')
	tr.validate(m1, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.046598080545663834, 'val_acc': 0.9889604449272156}]
	tr.validate(m1, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.4002395272254944, 'val_acc': 0.919881284236908}]

	* ABC: wandb/run-20230306_001846-3xaj8o6q: (e95afymd:drawn-wood-83)
	- Only able to get to %70 even with strong regularization:
	abctrn = ld.calvindataset2("../data/ABC-training")
	abcval = ld.calvindataset2("../data/ABC-validation")
	mlp.train(abctrn, abcval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.7, weight_decay=0.2, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/e95afymd/checkpoints/epoch=77-step=43602.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.6437634825706482, 'val_acc': 0.712051510810852}]
	tr.validate(m2, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 1.0606377124786377, 'val_acc': 0.6468842625617981}]

	* D: wandb/run-20230306_001635-p06ygu9d: (0xftsmbg:eternal-grass-82)
	- Pretty much all experiments converge to 93% for a range of regularizations.
	ABCD does significantly better on the same validation set.
	But D language annotations are different! And this time the order is reversed? Cooked val set for D-training?
	dtrn = ld.calvindataset2("../data/ABC-training")
	dval = ld.calvindataset2("../data/ABC-validation")
	mlp.train(dtrn, dval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/0xftsmbg/checkpoints/epoch=927-step=149408.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.3091912567615509, 'val_acc': 0.9153633713722229}]
	tr.validate(m3, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.25575390458106995, 'val_acc': 0.9426310658454895}]


	* === MLP-Experiments-32x1-frame: context_length=1, instances_per_episode=32
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)

	* ABCD:
	abcdtrn = ld.calvindataset1("../data/ABCD-training")
	abcdval = ld.calvindataset1("../data/ABCD-validation")
	dval = ld.calvindataset1("../data/D-validation")
	mlp.train(abcdtrn, abcdval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/iyyed7m1/checkpoints/epoch=5-step=137796.ckpt')
	tr.validate(m1, DataLoader(abcdval, batch_size=32)) #=> [{'val_loss': 0.4216434359550476, 'val_acc': 0.871147632598877}]
	tr.validate(m1, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 0.6235239505767822, 'val_acc': 0.8378152847290039}]

	* ABC:
	abctrn = ld.calvindataset1("../data/ABC-training")
	abcval = ld.calvindataset1("../data/ABC-validation")
	mlp.train(abctrn, abcval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/ondcx26m/checkpoints/epoch=5-step=107220.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.9127740859985352, 'val_acc': 0.6380232572555542}]
	tr.validate(m2, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 1.1524760723114014, 'val_acc': 0.6060212850570679}]

	* D:
	dtrn = ld.calvindataset1("../data/D-training")
	dval = ld.calvindataset1("../data/D-validation")
	mlp.train(dtrn, dval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/ltfpotls/checkpoints/epoch=27-step=143472.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.38017138838768005, 'val_acc': 0.8763511776924133}]
	tr.validate(m3, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.4305930435657501,  'val_acc': 0.8666542768478394}]


	* === MLP-Experiments-32x4-frame: context_length=4, instances_per_episode=32
	tr = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)

	* ABCD:
	abcdtrn = ld.calvindataset3("../data/ABCD-training")
	abcdval = ld.calvindataset3("../data/ABCD-validation")
	dval = ld.calvindataset3("../data/D-validation")
	mlp.train(abcdtrn, abcdval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m1 = mlp.LitMLP.load_from_checkpoint('mlp_project/sie028z1/checkpoints/epoch=5-step=137796.ckpt')
	tr.validate(m1, DataLoader(abcdval, batch_size=32)) #=> [{'val_loss': 0.24731987714767456, 'val_acc': 0.9218031167984009}]
	tr.validate(m1, DataLoader(dval, batch_size=32))    #=> [{'val_loss': 0.48957118391990660, 'val_acc': 0.8860039710998535}]

	* ABC:
	abctrn = ld.calvindataset3("../data/ABC-training")
	abcval = ld.calvindataset3("../data/ABC-validation")
	mlp.train(abctrn, abcval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m2 = mlp.LitMLP.load_from_checkpoint('mlp_project/1szhyy9m/checkpoints/epoch=2-step=53610.ckpt')
	tr.validate(m2, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.8835569024085999, 'val_acc': 0.6489190459251404}]
	tr.validate(m2, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 1.1324200630187988, 'val_acc': 0.6252781748771667}]

	* D:
	dtrn = ld.calvindataset3("../data/D-training")
	dval = ld.calvindataset3("../data/D-validation")
	mlp.train(dtrn, dval, max_steps=150000, batch_size=32, lr=0.0001, dropout=0.5, weight_decay=0.1, hidden=[512,512])
	m3 = mlp.LitMLP.load_from_checkpoint('mlp_project/7z3vxixb/checkpoints/epoch=26-step=138348.ckpt')
	tr.validate(m3, DataLoader(abcval, batch_size=32)) #=> [{'val_loss': 0.3023688793182373,  'val_acc': 0.8982003331184387}]
	tr.validate(m3, DataLoader(dval, batch_size=32))   #=> [{'val_loss': 0.38487085700035095, 'val_acc': 0.8820474743843079}]


	* bugs:
	+ the validation results do not match when reload from checkpoint!
	We need to call m.eval() before accuracy. Call m.train() again to activate dropout etc.
	+ the validation sets of D and ABC are different (same data different language. ABC=ABCD)
	Reporting both
	+ wandb: cannot see max validation accuracy!
	pytorch lightning: checkpoint model at best val: https://pytorch-lightning.readthedocs.io/en/stable/common/checkpointing_intermediate.html
	use checkpoint_callback = ModelCheckpoint(monitor = "val_acc", mode = 'max')
	pl.Trainer(..., callbacks=[checkpoint_callback], ...)


	* hyperparameters:
	>>> a = torch.load('mlp_project/v3vfpaws/checkpoints/epoch=178-step=100000.ckpt')
	>>> a["hyper_parameters"]
	{'sizes': (2336, 512, 512, 34), 'lr': 0.0001, 'weight_decay': 0.1, 'dropout': 0.6}


2023-03-04  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* mlp:
	+ 512 hidden 0.5 dropout gives 82% validation accuracy on D for single frame using all features.
	+ Using 2 or 3 frames only increase this less than 1%.
	+ Visualize the predictions to see what is going on.

	* system:
	+ error in process filter: No such directory found via CDPATH environment variable (emacs terminal with srun)
	  fixed disabling term-command-hook.

	* pl:
	+ forward vs predict_step: do we need both? forward is needed for predict.
	+ wandb init: can we log every run? currently starting python every time: need to call wandb.finish() at the end of experiment.


2023-03-02  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* wandb: Emre Can notes:
	Accuracy Metric code-line: https://github.com/emrecanacikgoz/robot-language/blob/2520cef611b115eb62938dc8859be0738f0a4946/blind_robot/models/mlp.py#L82
	training epoch ending function: https://github.com/emrecanacikgoz/robot-language/blob/2520cef611b115eb62938dc8859be0738f0a4946/blind_robot/models/mlp.py#L88
	my wandb logger: https://github.com/emrecanacikgoz/robot-language/blob/2520cef611b115eb62938dc8859be0738f0a4946/main.py#L93
	project argumanı yeni proje açıyor hocam
	name argumanı ilgili projenin içindeki run adı
	aynı proje için several run yapıcaksanız name'i değiştirebilirsiniz

	* TODO:
	- load checkpoint
	+ measure val/trn loss
	+ measure val/trn acc
	- tensorboard
	+ wandb: when do you get a new run?
	+ visualize training curves
	+ normalization
	x overfitting: usual methods did not work, try feature selection, try delta of scene coordinates, delta of robot obs, lower frame rate, multiple frames.


2023-02-23  dyuret  <dyuret@login03.kuacc.ku.edu.tr>

	* TODO:
	- validate rel_actions formula
	- extract tactile pixels
	- see if controller coordinates are in the simulator: https://github.com/mees/calvin_env
	- try mlp on action classification with different inputs

2023-02-22  dyuret  <dyuret@login02.kuacc.ku.edu.tr>

	* controller-coordinates: door, drawer, button, switch.


2023-02-19  Deniz Yuret  <dyuret@WS001>

	* scene_info.npy:
	# | debug/training | {'calvin_scene_D': [358482, 361252]} |
	# | debug/validation | {'calvin_scene_D': [553567, 555241]} |
	# | D/training | {'calvin_scene_A': [0, 611098]} |
	# | D/validation | . |
	# | ABC/training | {'calvin_scene_B': [0, 598909], 'calvin_scene_C': [598910, 1191338], 'calvin_scene_A': [1191339, 1795044]} |
	# | ABC/validation | . |
	# | ABCD/training | {'calvin_scene_A': [1802438, 2406143], 'calvin_scene_B': [611099, 1210008], 'calvin_scene_C': [1210009, 1802437], 'calvin_scene_D': [0, 611098]} |
	# | ABCD/validation | . |


	* TODO:
	+ write intervals.py to check intervals: done, several off-by-one errors in ABCD/training (frames common with validation).
	+ check overlaps see if identical: all identical but renumbered.
	+ check visual difference in A vs D: B is lightest, A is darkest, C and D similar with C having a straight pattern, D a bit lighter.
	+ see if ep_start_end is consistent with frame diffs
	+ improved visualizer to see what number is what
